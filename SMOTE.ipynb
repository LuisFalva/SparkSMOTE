{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autor: @LuisFalva\n",
    "\n",
    "#### *SMOTE* es una técnica para balancear datos. Normalmente, a la hora de entrenar un modelo tenemos que generar nuestra variable *target* [0,1] con la cual podremos calcular una predicción a partir de los registros observados, ¿pero que pasa cuando el 'target' que nos interesa es la clase minoritaria? Esto es un problema típico que muchos modelos sufren, dado que nuestra clase de interés será, en la mayoría de los casos, la clase minoritaria, tenemos que buscar una técnica para implementar un sobremuestreo sin perder información.\n",
    "\n",
    "<img src=\"src/smote.gif\" width=\"750\" align=\"center\">\n",
    "\n",
    "**img link: [The main issue with identifying Financial Fraud using Machine Learning (and how to address it)](https://towardsdatascience.com/the-main-issue-with-identifying-financial-fraud-using-machine-learning-and-how-to-address-it-3b1bf8fa1e0c)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dentro de este notebook, están las notas de estudio respecto a la técnica Synthetic Minority Oversampling Technique [SMOTE] la cual hace uso del algoritmo de k-NN para encontrar los vecinos más cercanos a la clase minoritaria, i.e. la clase de los positivos '1'. Se involucra el uso mixto de sklearn y pyspark, la idea de esta solución es publicar una forma de tantas posibles para implementar un método de muestreo de forma distribuida, por tal razón tenemos que definir a continuación la sesión de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SMOTE\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA IMPORTANTE: Para hacer uso de la función *smote_samplig()* solo tenemos que importar la clase SparkSmote y crear la instancia de la clase:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smote.spark_smote import SparkSmote\n",
    "\n",
    "spark_smote = SparkSmote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para la construcción de la función que nos ayudará a generar nuestras muestras sintéticas, vamos a cargar la tabla **\"src/data/\"**, la cual contiene una cantidad de variables que describen las caracteristicas principales de un cliente por cada renglón. El dataframe que usaremos mantendrá de origen las siguientes variables numéricas:\n",
    "- **[age, child, saving, insight, backup, marital]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-------+------+-------+\n",
      "|age|child|saving|insight|backup|marital|\n",
      "+---+-----+------+-------+------+-------+\n",
      "|59 |1    |0     |1      |1     |married|\n",
      "|56 |0    |1     |0      |1     |married|\n",
      "|41 |1    |1     |0      |0     |married|\n",
      "|55 |1    |0     |0      |1     |married|\n",
      "|54 |1    |0     |0      |1     |married|\n",
      "+---+-----+------+-------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arr_col = [\"age\", \"child\", \"saving\", \"insight\", \"backup\", \"marital\"]\n",
    "smote_test = spark.read.parquet(\"src/data/\").select(*arr_col)\n",
    "smote_test.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**El set de datos contiene la variable 'marital', en dicha variable existen 3 clases: [married, single, divorced], por conveniencia al ejemplo que se contruye a lo largo de este notebook, se ha elegido la clase 'divorced' como nuestro target, podemos notar claramente que existe un desbalance considerable a la hora de contar la cantidad de registros que satisfacen nuestro público target, i.e. la clase 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|target|count|\n",
      "+------+-----+\n",
      "|     1| 1293|\n",
      "|     0| 9869|\n",
      "+------+-----+\n",
      "\n",
      "+---+-----+------+-------+------+------+\n",
      "|age|child|saving|insight|backup|target|\n",
      "+---+-----+------+-------+------+------+\n",
      "| 60|    0|     1|      0|     0|     1|\n",
      "| 35|    0|     1|      1|     1|     1|\n",
      "| 49|    1|     1|      1|     0|     1|\n",
      "| 28|    0|     0|      0|     0|     1|\n",
      "| 43|    1|     1|      0|     1|     1|\n",
      "+---+-----+------+-------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = smote_test.select(\"*\", (when(col(\"marital\") == \"divorced\", 1).otherwise(0)).alias(\"target\")).drop(\"marital\")\n",
    "test.groupBy(\"target\").count().show()\n",
    "test.where(col(\"target\") == 1).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ahora bien, lo que se busca para entrenar un modelo de k vecinos cercanos [i.e. k-NN], por ejemplo, es un objeto de tipo numpy.array con los valores de cada registro, algo similar a esto:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[60,  0,  1,  0,  0],\n",
       "       [35,  0,  1,  1,  1],\n",
       "       [49,  1,  1,  1,  0],\n",
       "       ...,\n",
       "       [52,  0,  0,  0,  0],\n",
       "       [38,  0,  1,  0,  1],\n",
       "       [60,  1,  1,  0,  1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(test.where(col(\"target\") == 1).drop(\"target\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA: Sin embargo, para convertir de un Spark Dataframe a un objeto de tipo numpy.array es conveniente antes transformarlo a RDD, por lo que los métodos de la clase SparkSMOTE se encargarán de realizar internamente esos parseos.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El algoritmo de k-NN necesita recibir como entrada un objeto de tipo np.array por lo que se debe convertir nuestro spark Dataframe a un objeto de tipo numpy array, y para ello tenemos que convertir nuestro spark dataframe a rdd's, para que la estructura de datos al ser transformada ésta sea de manera distribuida.\n",
    "\n",
    "#### Para generar nuestras muestras sintéticas debemos antes vectorizar los atributos que tengamos en nuestra tabla de datos, esto significa que debemos tomar los valores de cada columna y crear vectores de longitud **$p$**. El método **smote_sampling** asume tres principales puntos:\n",
    "\n",
    "- Normalización y estandarización de variables\n",
    "- Mapeo de cada valor por columna a codificaciones binarias (StringIndexer, OneHotEncoder)\n",
    "- Spark Dataframe vectorizado, i.e. con columna de vectores densos y escasos (features), y columna dicotómica (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+\n",
      "|features              |label|\n",
      "+----------------------+-----+\n",
      "|[59.0,1.0,0.0,1.0,1.0]|0    |\n",
      "|[56.0,0.0,1.0,0.0,1.0]|0    |\n",
      "|[41.0,1.0,1.0,0.0,0.0]|0    |\n",
      "|[55.0,1.0,0.0,0.0,1.0]|0    |\n",
      "|[54.0,1.0,0.0,0.0,1.0]|0    |\n",
      "+----------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_assemble = spark_smote.vector_assembling(test, \"target\")\n",
    "vector_assemble.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como muestra de su funcionamiento, para aplicar el método *smote_sampling* requerimos de la tabla anterior con variables previamente standarizados, codificados y vectorizados. Como se puede ver, el método recibe los argumentos 'pct_over_min' y 'pct_under_max' configurados por default en [100, 100] respectivamente, cada uno de esos argumentos ayudarán a manipular el submuestreo o sobremuestreo de ambas clases que se ven en la siguiente tabla.\n",
    "\n",
    "- **pct_over_min; modificará la cantidad de registros que existe para la clase minoritaria sobremuestreando los registros con valores sintéticos, en este caso, la clase '1'**\n",
    "\n",
    "- **pct_under_max; modificará la cantidad de registros que existe para la clase mayoritaria submuestreando los registros, en este caso, la clase '0'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0| 9869|\n",
      "|    1| 9051|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1236.778682061702, 1235.37473148036, 1235.414...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1233.0239286735316, 1232.6035057438876, 1232....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1231.2575290198367, 1231.1173686630991, 1231....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1229.467505661976, 1229.0195860538968, 1229.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1223.9368066679888, 1222.459148073263, 1222.4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  label\n",
       "0  [1236.778682061702, 1235.37473148036, 1235.414...      1\n",
       "1  [1233.0239286735316, 1232.6035057438876, 1232....      1\n",
       "2  [1231.2575290198367, 1231.1173686630991, 1231....      1\n",
       "3  [1229.467505661976, 1229.0195860538968, 1229.0...      1\n",
       "4  [1223.9368066679888, 1222.459148073263, 1222.4...      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smote_sample = spark_smote.smote_sampling(spark, vector_assemble, pct_over_min=600, pct_under_max=100)\n",
    "smote_sample.groupBy(\"label\").count().show()\n",
    "smote_sample.where(col(\"label\") == 1).orderBy(col(\"features\").desc()).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Referencias:\n",
    "- https://rikunert.com/SMOTE_explained\n",
    "- https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-106\n",
    "- https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
