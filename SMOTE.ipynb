{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autor: @LuisFalva\n",
    "\n",
    "### SMOTE es una técnica para balancear datos. Normalmente, a la hora de entrenar un modelo tenemos que generar nuestra variable *target* [0,1] con el cual podremos calcular una predicción apartir de los registros observados, ¿pero que pasa cuando el 'target' que nos interesa es la clase minoritaria? Esto es un problema típico que muchos modelos sufren, dado que nuestra clase de interés será, en la mayoría de los casos, la clase minoritaria, tenemos que buscar una técnica para implementar un sobremuestreo sin perder información.\n",
    "\n",
    "<img src=\"src/smote.gif\" width=\"750\" align=\"center\">\n",
    "\n",
    "### Dentro de este notebook, están las notas de estudio respecto a la técnica Synthetic Minority Oversampling Technique [SMOTE] la cual hace uso del algoritmo de k-NN para encontrar los vecinos más cercanos a la clase minoritaria, i.e. la clase de los positivos '1'.\n",
    "\n",
    "**img link: [The main issue with identifying Financial Fraud using Machine Learning (and how to address it)](https://towardsdatascience.com/the-main-issue-with-identifying-financial-fraud-using-machine-learning-and-how-to-address-it-3b1bf8fa1e0c)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import neighbors\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Este notebook involucra el uso mixto de sklearn y pyspark, la idea de esta solución es publicar una forma de tantas posibles para implementar un método de muestreo de forma distribuida, por tal razón tenemos que definir a continuación el contexto de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SMOTE\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para la construcción de la función que nos ayudará a generar nuestras muestras sintéticas, vamos a cargar la tabla **\"src/data/\"**, la cual contiene una cantidad de variables que describen las caracteristicas principales de un cliente por cada renglón. El dataframe que usaremos mantendrá de origen las siguientes variables numéricas:\n",
    "- **[age, child, saving, insight, backup, marital]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-------+------+-------+\n",
      "|age|child|saving|insight|backup|marital|\n",
      "+---+-----+------+-------+------+-------+\n",
      "|59 |1    |0     |1      |1     |married|\n",
      "|56 |0    |1     |0      |1     |married|\n",
      "|41 |1    |1     |0      |0     |married|\n",
      "|55 |1    |0     |0      |1     |married|\n",
      "|54 |1    |0     |0      |1     |married|\n",
      "+---+-----+------+-------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arr_col = [\"age\", \"child\", \"saving\", \"insight\", \"backup\", \"marital\"]\n",
    "smote_test = spark.read.parquet(\"src/data/\").select(*arr_col)\n",
    "smote_test.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|target|count|\n",
      "+------+-----+\n",
      "|     1| 1293|\n",
      "|     0| 9869|\n",
      "+------+-----+\n",
      "\n",
      "+---+-----+------+-------+------+------+\n",
      "|age|child|saving|insight|backup|target|\n",
      "+---+-----+------+-------+------+------+\n",
      "| 60|    0|     1|      0|     0|     1|\n",
      "| 35|    0|     1|      1|     1|     1|\n",
      "| 49|    1|     1|      1|     0|     1|\n",
      "| 28|    0|     0|      0|     0|     1|\n",
      "| 43|    1|     1|      0|     1|     1|\n",
      "+---+-----+------+-------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = smote_test.select(\"*\", (when(col(\"marital\") == \"divorced\", 1).otherwise(0)).alias(\"target\")).drop(\"marital\")\n",
    "test.groupBy(\"target\").count().show()\n",
    "test.where(col(\"target\") == 1).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lo que nosotros buscamos para entrenar nuestro modelo de k vecinos cercanos [i.e. k-NN] es un objeto de tipo numpy array con los valores de cada registro, algo similar a esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[60,  0,  1,  0,  0],\n",
       "       [35,  0,  1,  1,  1],\n",
       "       [49,  1,  1,  1,  0],\n",
       "       ...,\n",
       "       [52,  0,  0,  0,  0],\n",
       "       [38,  0,  1,  0,  1],\n",
       "       [60,  1,  1,  0,  1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test.where(col(\"target\") == 1).drop(\"target\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA: Sin embargo, para convertir de un Spark Dataframe a un objeto de tipo numpy.array es conveniente antes transformarlo a RDD, por lo que los métodos de las clase SparkSMOTE realizarán internamente esos parseos.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para entrenar el modelo de k-NN necesitaremos convertir nuestro spark Dataframe a un objeto de tipo numpy array, y para ello debemos bajar nuestra estructura dataframe a rdd's, para que la estructura de datos al ser transformada ésta sea de manera distribuida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_assembling(data_input, target_name):\n",
    "    \"\"\"\n",
    "    Vector assembling function will create a vector filled with features for each row\n",
    "    \n",
    "    :param data_input: df, spark Dataframe with target label\n",
    "    :param target_name: str, string name from target label\n",
    "    :return: Dataframe, table that includes the feature vector and label\n",
    "    \"\"\"\n",
    "    \n",
    "    if data_input.select(target_name).distinct().count() != 2:\n",
    "        raise ValueError(\"Target field must have only 2 distinct classes\")\n",
    "    \n",
    "    column_names = list(data_input.drop(target_name).columns)\n",
    "    vec_assembler = VectorAssembler(inputCols=column_names, outputCol='features')\n",
    "    vec_transform = vec_assembler.transform(data_input)\n",
    "    vec_feature = vec_transform.select('features', (vec_transform[target_name]).alias(\"label\"))\n",
    "    \n",
    "    return vec_feature\n",
    "\n",
    "def split_target(df, field, minor=1, major=0):\n",
    "    \"\"\"\n",
    "    Split target will split in two distinct Dataframe from label 1 and 0\n",
    "    \n",
    "    :param df: Dataframe, spark Dataframe with target label\n",
    "    :param field: str, string name from taget label\n",
    "    :param minor: int, integer number for minority class; '1' set as default\n",
    "    :param major: int, integer number for majority class; '0' set as default\n",
    "    :return: dict, python dictionary with separated Dataframe\n",
    "    \"\"\"\n",
    "    minor = df[df[field] == minor]\n",
    "    major = df[df[field] == major]\n",
    "    \n",
    "    return {\"minor\": minor, \"major\": major}\n",
    "\n",
    "def spkdf_to_nparr(df, feature):\n",
    "    \"\"\"\n",
    "    Spkdf to nparr function will help to parse from spark Dataframe to numpy array\n",
    "    in a distributed way\n",
    "    \n",
    "    :param df: Dataframe, spark Dataframe with features column\n",
    "    :param feature: str, string name of column features name\n",
    "    :return: np.array, numpy array object with features\n",
    "    \"\"\"\n",
    "    feature_df = df.select(feature)\n",
    "    \n",
    "    return np.asarray(feature_df.rdd.map(lambda x: x[0]).collect())\n",
    "\n",
    "def nparr_to_spkdf(spark_session, arr, feat=\"features\", label=\"label\"):\n",
    "    \"\"\"\n",
    "    Nparr to spkdf function will help to parse from numpy array to spark Dataframe\n",
    "    in a distributed way\n",
    "    \n",
    "    :param df: Dataframe, spark Dataframe with features column\n",
    "    :param feat: str, string name of column features name; 'features' set as default\n",
    "    :param label: str, string name of column label name; 'label' set as default\n",
    "    :return: Dataframe, with feautures and label\n",
    "    \"\"\"\n",
    "    sc = spark_session.sparkContext\n",
    "    data_set = sc.parallelize(arr)\n",
    "    data_rdd = data_set.map(lambda x: (Row(fatures=DenseVector(x), label=1)))\n",
    "    \n",
    "    return data_rdd.toDF()\n",
    "\n",
    "def smote_sampling(spark, df, k=2, algth=\"auto\", pct_over_min=100, pct_under_max=100):\n",
    "    \"\"\"\n",
    "    Smote sampling function will create an oversampling with SMOTE technique\n",
    "    \n",
    "    :param df: Dataframe, spark Dataframe with features column\n",
    "    :param k: int, integer k folds for KNN's groups; '2' set as default\n",
    "    :param algrth: str, string name for KNN's algorithm choice; 'auto' set as default\n",
    "    :param pct_over_min: int, integer number for sampling minority class; '100' set as default\n",
    "    :param pct_under_max: int, integer number for sampling majority class; '100' set as default\n",
    "    :return: Dataframe, with new SMOTE features sampled\n",
    "    \"\"\"\n",
    "    def k_neighbor(k, algth, feature):\n",
    "        \"\"\"\n",
    "        k neighbor will compute Nearest Neighbors sklearn algorithm\n",
    "\n",
    "        :param k: int, integer number for k nearest neighbors groups\n",
    "        :param feature: str, string name of column features name\n",
    "        :return: list, python list with numpy array object for each neighbor\n",
    "        \"\"\"\n",
    "        neighbor = neighbors.NearestNeighbors(n_neighbors=k, algorithm=algth)\n",
    "        model_fit = neighbor.fit(feature)\n",
    "        \n",
    "        return model_fit.kneighbors(feature)\n",
    "    \n",
    "    def compute_smo(neighbor_list, min_pct, min_arr):\n",
    "        \"\"\"\n",
    "        Compute smo function will compute the SMOTE oversampling technique\n",
    "\n",
    "        :param neighbor_list: list, python list with numpy array object for each neighbor\n",
    "        :param min_pct: int, integer pct for over min\n",
    "        :param min_arr: list, python list with minority class rows\n",
    "        :param k: int, integer number for k nearest neighbors groups\n",
    "        :return: list, python list with sm class oversampled\n",
    "        \"\"\"\n",
    "        if min_pct < 100:\n",
    "            raise ValueError(\"pct_over_min can't be less than 100\")\n",
    "        \n",
    "        smo = []\n",
    "        counter = 0\n",
    "        pct_over = int(min_pct / 100)\n",
    "        \n",
    "        while len(min_arr) > counter:\n",
    "            for i in range(pct_over):\n",
    "                random_neighbor = random.randint(0, len(neighbor)-1)\n",
    "                diff = neighbor_list[random_neighbor][0] - min_arr[i][0]\n",
    "                new_record = (min_arr[i][0] + random.random() * diff)\n",
    "                smo.insert(0, (new_record))\n",
    "            counter+=1\n",
    "            \n",
    "        return smo\n",
    "    \n",
    "    data_input_min = split_target(df=df, field=\"label\")[\"minor\"]\n",
    "    data_input_max = split_target(df=df, field=\"label\")[\"major\"]\n",
    "    \n",
    "    feature_mat = spkdf_to_nparr(df=data_input_min, feature=\"features\")\n",
    "    neighbor = k_neighbor(k=k, algth=algth, feature=feature_mat)[1]\n",
    "    \n",
    "    min_array = data_input_min.drop(\"label\").rdd.map(lambda x : list(x)).collect()\n",
    "    new_row = compute_smo(neighbor_list=neighbor, min_pct=pct_over_min, min_arr=min_array)\n",
    "    smo_data_df = nparr_to_spkdf(spark_session=spark, arr=new_row)\n",
    "    smo_data_minor = data_input_min.unionAll(smo_data_df)\n",
    "    \n",
    "    if (pct_under_max < 10) | (pct_under_max > 100):\n",
    "        raise ValueError(\"pct_under_max can't be less than 10 either higher than 100\")\n",
    "    new_data_major = data_input_max.sample(False, (float(pct_under_max / 100)))\n",
    "    \n",
    "    return new_data_major.unionAll(smo_data_minor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para computar nuestras muestras sintéticas debemos antes vectorizar los atributos que tengamos en nuestra tabla de datos, esto significa que debemos tomar los valores de cada columna y crear vectores de longitud **$p$**. Este método asume tres principales puntos:\n",
    "\n",
    "- Normalización y estandarización de variables\n",
    "- Mapeo de cada valor por columna a codificaciones binarias (StringIndexer, OneHotEncoder)\n",
    "- Spark Dataframe vectorizado, i.e. con columna de vectores densos y escasos (features), y columna dicotómica (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+\n",
      "|features              |label|\n",
      "+----------------------+-----+\n",
      "|[59.0,1.0,0.0,1.0,1.0]|0    |\n",
      "|[56.0,0.0,1.0,0.0,1.0]|0    |\n",
      "|[41.0,1.0,1.0,0.0,0.0]|0    |\n",
      "|[55.0,1.0,0.0,0.0,1.0]|0    |\n",
      "|[54.0,1.0,0.0,0.0,1.0]|0    |\n",
      "+----------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_assemble = vector_assembling(test, \"target\")\n",
    "vector_assemble.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como muestra de su funcionamiento, para aplicar el método *smote_sampling* requerimos de la tabla anterior con variables previamente standarizados, codificados y vectorizados. Como se puede ver, el método recibe los argumentos 'pct_over_min' y 'pct_under_max' configurados por default en [100, 100] respectivamente, cada uno de esos argumentos ayudarán a manipular el submuestreo o sobremuestreo de ambas clases que se ven en la siguiente tabla.\n",
    "\n",
    "**pct_over_min; modificará la cantidad de registros que existe para la clase minoritaria sobremuestreando los registros con valores sintéticos, en este caso, la clase '1'**\n",
    "\n",
    "**pct_under_max; modificará la cantidad de registros que existe para la clase mayoritaria submuestreando los registros, en este caso, la clase '0'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0| 9869|\n",
      "|    1| 9051|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1271.0918104712482, 1270.6470927128566, 1270....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1252.0007268819193, 1251.6117668010615, 1251....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1248.406327167742, 1248.385651994479, 1248.38...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1247.931361874167, 1247.8941456210284, 1247.8...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1230.4068770858046, 1229.7593553922754, 1229....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  label\n",
       "0  [1271.0918104712482, 1270.6470927128566, 1270....      1\n",
       "1  [1252.0007268819193, 1251.6117668010615, 1251....      1\n",
       "2  [1248.406327167742, 1248.385651994479, 1248.38...      1\n",
       "3  [1247.931361874167, 1247.8941456210284, 1247.8...      1\n",
       "4  [1230.4068770858046, 1229.7593553922754, 1229....      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smote_sample = smote_sampling(spark, vector_assemble, pct_over_min=600, pct_under_max=100)\n",
    "smote_sample.groupBy(\"label\").count().show()\n",
    "smote_sample.where(col(\"label\") == 1).orderBy(col(\"features\").desc()).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Referencias:\n",
    "- https://rikunert.com/SMOTE_explained\n",
    "- https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-106\n",
    "- https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
