{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autor: @LuisFalva\n",
    "\n",
    "## SMOTE es una técnica para balancear datos, normalmente, a la hora de entrenar un modelo buscamos generar nuestro target con el cual podemos computar una predicción apartir de ellos, ¿pero que pasa cuando nuestro target es muy pequeño? Esto es un problema típico que muchos modelos sufren, dado que nuestra clase de interés será, en la mayoría de los casos, la clase minoritaria, tenemos que buscar una técnica para implementar un sobremuestreo inteligente.\n",
    "\n",
    "## Dentro de este notebook, están las notas de estudio respecto a la técnica Synthetic Minority Oversampling Technique [SMOTE] la cual hace uso del algoritmo de KNN para encontrar los vecinos más cercanos a la clase minoritaria, i.e. la clase de los positivos '1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output_subarea.output_text.output_stream.output_stdout > pre {\n",
       "    width:max-content;\n",
       "}\n",
       ".p-Widget.jp-RenderedText.jp-OutputArea-output > pre {\n",
       "   width:max-content;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_subarea.output_text.output_stream.output_stdout > pre {\n",
    "    width:max-content;\n",
    "}\n",
    ".p-Widget.jp-RenderedText.jp-OutputArea-output > pre {\n",
    "   width:max-content;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import neighbors\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SMOTE\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para la construcción de la función que nos ayudará a generar nuestras muestras sintéticas, vamos a cargar la tabla 'smote_class', la cual contiene una cantidad de variables que describen las caracteristicas principales de un cliente por cada renglón. El dataframe que usaremos para mantendrá únicamente las variables numéricas:\n",
    "- [age, child, saving, insight, backup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-------+------+-------+\n",
      "|age|child|saving|insight|backup|marital|\n",
      "+---+-----+------+-------+------+-------+\n",
      "|59 |1    |0     |1      |1     |married|\n",
      "|56 |0    |1     |0      |1     |married|\n",
      "|41 |1    |1     |0      |0     |married|\n",
      "|55 |1    |0     |0      |1     |married|\n",
      "|54 |1    |0     |0      |1     |married|\n",
      "+---+-----+------+-------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arr_col = [\"age\", \"child\", \"saving\", \"insight\", \"backup\", \"marital\"]\n",
    "smote_test = spark.read.parquet(\"smote_class/\").select(*arr_col)\n",
    "smote_test.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|target|count|\n",
      "+------+-----+\n",
      "|     1| 1293|\n",
      "|     0| 9869|\n",
      "+------+-----+\n",
      "\n",
      "+---+-----+------+-------+------+------+\n",
      "|age|child|saving|insight|backup|target|\n",
      "+---+-----+------+-------+------+------+\n",
      "| 59|    1|     0|      1|     1|     0|\n",
      "| 56|    0|     1|      0|     1|     0|\n",
      "| 41|    1|     1|      0|     0|     0|\n",
      "| 55|    1|     0|      0|     1|     0|\n",
      "| 54|    1|     0|      0|     1|     0|\n",
      "+---+-----+------+-------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = smote_test.select(\"*\", (when(col(\"marital\") == \"divorced\", 1).otherwise(0)).alias(\"target\")).drop(\"marital\")\n",
    "test.groupBy(\"target\").count().show()\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lo que nosotros buscamos para entrenar nuestro modelo de vecinos cercanos [i.e. KNN] es un objeto de tipo numpy array con los valores de cada registro, algo similar a esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[60,  0,  1,  0,  0],\n",
       "       [35,  0,  1,  1,  1],\n",
       "       [49,  1,  1,  1,  0],\n",
       "       ...,\n",
       "       [52,  0,  0,  0,  0],\n",
       "       [38,  0,  1,  0,  1],\n",
       "       [60,  1,  1,  0,  1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test.where(col(\"target\") == 1).drop(\"target\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para entrenar el modelo de KNN necesitaremos convertir nuestro spark Dataframe a un objeto de tipo numpy array, y para ello debemos bajar nuestra estructura dataframe a rdd's para que las estructura de datos al ser transformada ésta sea de manera distribuida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_assembling(data_input, target_name):\n",
    "    \"\"\"\n",
    "    Vectorizer function will create a vector filled with features for each row\n",
    "    \n",
    "    :param data_input: df, spark Dataframe with target label\n",
    "    :param target_name: str, string name from target label\n",
    "    :return: Dataframe, table that includes the feature vector and label\n",
    "    \"\"\"\n",
    "    \n",
    "    if data_input.select(target_name).distinct().count() != 2:\n",
    "        raise ValueError(\"Target field must have only 2 distinct classes\")\n",
    "    \n",
    "    column_names = list(data_input.drop(target_name).columns)\n",
    "    vector_assembler = VectorAssembler(inputCols = column_names, outputCol = 'features')\n",
    "    vector_transform = vector_assembler.transform(data_input)\n",
    "    vector_feature = vector_transform.select('features', (vector_transform[target_name]).alias(\"label\"))\n",
    "    \n",
    "    return vector_feature\n",
    "\n",
    "def split_target(df, field, minor=1, major=0):\n",
    "    \"\"\"\n",
    "    Split target will split in two distinct Dataframe from label 1 and 0\n",
    "    \n",
    "    :param df: Dataframe, spark Dataframe with target label\n",
    "    :param field: str, string name from taget label\n",
    "    :param minor: int, integer number for minority class\n",
    "    :param major: int, integer number for majority class\n",
    "    :return: dict, python dictionary with separated Dataframe\n",
    "    \"\"\"\n",
    "    minor = df[df[field] == minor]\n",
    "    major = df[df[field] == major]\n",
    "    return {\"minor\": minor, \"major\": major}\n",
    "\n",
    "def spkdf_to_nparr(df, feature):\n",
    "    \"\"\"\n",
    "    Spkdf to nparr function will help to parse from spark Dataframe to numpy array\n",
    "    in a distributed way\n",
    "    \n",
    "    :param df: Dataframe, spark Dataframe with features column\n",
    "    :param feature: str, string name of column features name\n",
    "    :return: np.array, numpy array object with features\n",
    "    \"\"\"\n",
    "    feature_df = df.select(feature)\n",
    "    return np.asarray(feature_df.rdd.map(lambda x: x[0]).collect())\n",
    "\n",
    "def nparr_to_spkdf(arr, feat=\"features\", label=\"label\"):\n",
    "    \"\"\"\n",
    "    Nparr to spkdf function will help to parse from numpy array to spark Dataframe\n",
    "    in a distributed way\n",
    "    \n",
    "    :param df: Dataframe, spark Dataframe with features column\n",
    "    :param feat: str, string name of column features name; 'features' set as default\n",
    "    :param label: str, string name of column label name; 'label' set as default\n",
    "    :return: Dataframe, with feautures and label\n",
    "    \"\"\"\n",
    "    data_set = sc.parallelize(arr)\n",
    "    data_rdd = data_set.map(lambda x: Row(feat=x, label=1))\n",
    "    return data_rdd.toDF()\n",
    "\n",
    "def smote_sampling(df, k=2, algrth=\"auto\", minority_class=1, majority_class=0, pct_over_min=100, pct_under_max=100):\n",
    "    \"\"\"\n",
    "    Smote sampling function will create an oversampling with SMOTE technique\n",
    "    \n",
    "    :param df: Dataframe, spark Dataframe with features column\n",
    "    :param k: int, integer k folds for KNN's groups; '2' set as default\n",
    "    :param algrth: str, string name for KNN's algorithm choice; 'auto' set as default\n",
    "    :param minority_class: int, value related to minority class; '1' set as default\n",
    "    :param majority_class: int, value related to majority class; '0' set as default\n",
    "    :param pct_over_min: int, integer number for sampling minority class; '100' set as default\n",
    "    :param pct_under_max: int, integer number for sampling majority class; '100' set as default\n",
    "    :return: Dataframe, with new SMOTE features sampled\n",
    "    \"\"\"\n",
    "    def k_neighbor(k, feature):\n",
    "        \"\"\"\n",
    "        k neighbor will compute Nearest Neighbors sklearn algorithm\n",
    "\n",
    "        :param k: int, integer number for k nearest neighbors groups\n",
    "        :param feature: str, string name of column features name\n",
    "        :return: list, python list with numpy array object for each neighbor\n",
    "        \"\"\"\n",
    "        neighbor_list = neighbors.NearestNeighbors(n_neighbors=k, algorithm=algrth).fit(feature)\n",
    "        return neighbor_list.kneighbors(feature)\n",
    "    \n",
    "    def compute_smo(neighbor_list, pct, min_arr, k):\n",
    "        \"\"\"\n",
    "        Compute smo function will compute the SMOTE oversampling technique\n",
    "\n",
    "        :param neighbor_list: list, python list with numpy array object for each neighbor\n",
    "        :param pct: int, integer pct for over min\n",
    "        :param min_arr: list, python list with minority class rows\n",
    "        :param k: int, integer number for k nearest neighbors groups\n",
    "        :return: list, python list with sm class oversampled\n",
    "        \"\"\"\n",
    "        if pct < 100:\n",
    "            raise ValueError(\"Percentage Over Min must be in at least >= 100\")\n",
    "        \n",
    "        smo = []\n",
    "        counter = 0\n",
    "        pct_over = int(pct / 100)\n",
    "        \n",
    "        while len(min_arr) > counter:\n",
    "            for i in range(pct_over):\n",
    "                random_neighbor = random.randint(1, k)\n",
    "                diff = min_arr[random_neighbor][0] - min_arr[i][0]\n",
    "                new_record = (neighbor_list[i][0] + random.random() * diff)\n",
    "                smo.insert(0, (new_record))\n",
    "            counter+=1\n",
    "        \n",
    "        return smo\n",
    "    \n",
    "    data_input_min = split_target(df=df, field=\"label\")[\"minor\"]\n",
    "    data_input_max = split_target(df=df, field=\"label\")[\"major\"]\n",
    "    \n",
    "    feature_mat = spkdf_to_nparr(data_input_min, \"features\")\n",
    "    neighbor = k_neighbor(k=k, feature=feature_mat)[1]\n",
    "    \n",
    "    min_array = data_input_min.drop(\"label\").rdd.map(lambda x : list(x)).collect()\n",
    "    new_row = compute_smo(neighbor, pct_over_min, min_array, k)\n",
    "    smo_data_df = nparr_to_spkdf(new_row)\n",
    "    smo_data_minor = data_input_min.unionAll(smo_data_df)\n",
    "    new_data_major = data_input_max.sample(False, (float(pct_under_max / 100)))\n",
    "    \n",
    "    return new_data_major.unionAll(smo_data_minor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para computar nuestras muestras sintéticas debemos antes vectorizar los atributos que tengamos en nuestra tabla de datos, esto significa que debemos tomar los valores de cada columna y crear vectores de longitud **$p$**. Este método asume que las variables vienen standarizadas y los valores vienen codificados a números binarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+\n",
      "|features              |label|\n",
      "+----------------------+-----+\n",
      "|[59.0,1.0,0.0,1.0,1.0]|0    |\n",
      "|[56.0,0.0,1.0,0.0,1.0]|0    |\n",
      "|[41.0,1.0,1.0,0.0,0.0]|0    |\n",
      "|[55.0,1.0,0.0,0.0,1.0]|0    |\n",
      "|[54.0,1.0,0.0,0.0,1.0]|0    |\n",
      "+----------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_assemble = vector_assembling(test, \"target\")\n",
    "vector_assemble.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como muestra de su funcionamiento, para aplicar el método 'smote_sampling' requerimos de la tabla anterior con variables previamente standarizados, codificados y vectorizados. Como se puede ver, el método recibe los argumentos 'pct_over_min' y 'pct_under_max' configurados por default en [100, 100] respectivamente, cada uno de esos argumentos ayudarán a manipular el submuestreo o sobremuestreo de ambas clases que se ven en la siguiente tabla.\n",
    "\n",
    "## pct_over_min; modificará la cantidad de registros que existe para la clase minoritaria sobremuestreando los registros con valores sintéticos, en este caso, la clase '1'\n",
    "## pct_under_max; modificará la cantidad de registros que existe para la clase mayoritaria submuestreando los registros, en este caso, la clase '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0| 9869|\n",
      "|    1| 9051|\n",
      "+-----+-----+\n",
      "\n",
      "+----------------------+-----+\n",
      "|features              |label|\n",
      "+----------------------+-----+\n",
      "|[59.0,1.0,0.0,1.0,1.0]|0    |\n",
      "|[56.0,0.0,1.0,0.0,1.0]|0    |\n",
      "|[41.0,1.0,1.0,0.0,0.0]|0    |\n",
      "|[55.0,1.0,0.0,0.0,1.0]|0    |\n",
      "|[54.0,1.0,0.0,0.0,1.0]|0    |\n",
      "+----------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smote_sample = smote_sampling(vector_assemble, pct_over_min=600, pct_under_max=100)\n",
    "smote_sample.groupBy(\"label\").count().show()\n",
    "smote_sample.show(5, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
